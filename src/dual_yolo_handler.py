"""
Project-Cortex v2.0 - Dual YOLO Handler (Layer 0 + Layer 1)

This module orchestrates parallel inference of:
- Layer 0 (Guardian): YOLO11x for safety-critical static detection
- Layer 1 (Learner): YOLOE-11s for adaptive context-aware detection

Both models run CONCURRENTLY on the same frame using ThreadPoolExecutor.

ARCHITECTURE:
    Frame ‚Üí [Thread 1: YOLO11x Guardian, Thread 2: YOLOE Learner] ‚Üí Results

PERFORMANCE:
    - Layer 0: 60-80ms (safety path, <100ms requirement)
    - Layer 1: 90-130ms (context path, acceptable latency)
    - Total: ~90-130ms (parallel, not sequential)

INNOVATION:
    - First AI wearable with dual-model adaptive learning
    - Layer 1 learns from Gemini, Google Maps, User Memory (no retraining!)
    - Dynamic text prompts update every 30 seconds with <50ms overhead

Author: Haziq (@IRSPlays)
Competition: Young Innovators Awards (YIA) 2026
"""

import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional, Dict, List, Tuple, Any
import numpy as np

from layer0_guardian import YOLOGuardian
from layer1_learner import YOLOELearner, AdaptivePromptManager

logger = logging.getLogger(__name__)


class DualYOLOHandler:
    """
    Orchestrates parallel inference of Layer 0 (Guardian) and Layer 1 (Learner).
    
    Layer 0: YOLO11x (static, safety-critical, <100ms)
    Layer 1: YOLOE-11s (adaptive, contextual, <150ms)
    
    Both models process the same frame simultaneously via ThreadPoolExecutor.
    """
    
    def __init__(
        self,
        guardian_model_path: str = "models/yolo11x.pt",
        learner_model_path: str = "models/yoloe-11s-seg.pt",
        device: str = "cpu",
        max_workers: int = 2
    ):
        """
        Initialize dual-model system.
        
        Args:
            guardian_model_path: Path to YOLO11x weights (Layer 0)
            learner_model_path: Path to YOLOE-11s weights (Layer 1)
            device: Inference device ('cpu' or 'cuda')
            max_workers: Number of parallel threads (default: 2)
        """
        logger.info("üß† Initializing Dual YOLO Handler (Layer 0 + Layer 1)...")
        
        # Initialize Layer 0: Guardian (static safety model)
        logger.info("üõ°Ô∏è Loading Layer 0 Guardian (YOLO11x)...")
        self.guardian = YOLOGuardian(
            model_path=guardian_model_path,
            device=device
        )
        
        # Initialize Layer 1: Learner (adaptive context model)
        logger.info("üéØ Loading Layer 1 Learner (YOLOE-11s)...")
        self.learner = YOLOELearner(
            model_path=learner_model_path,
            device=device
        )
        
        # Initialize adaptive prompt manager
        self.prompt_manager = AdaptivePromptManager(
            max_prompts=50,  # Keep list under 50-100 classes
            storage_path="memory/adaptive_prompts.json"
        )
        
        # ThreadPoolExecutor for parallel inference
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
        # Performance tracking
        self.guardian_times = []
        self.learner_times = []
        self.total_times = []
        
        # Update learner with initial prompts
        self._update_learner_prompts()
        
        logger.info("‚úÖ Dual YOLO Handler initialized")
        logger.info(f"   Layer 0: {guardian_model_path} (static, {len(self.guardian.get_classes())} classes)")
        logger.info(f"   Layer 1: {learner_model_path} (adaptive, {len(self.prompt_manager.get_current_prompts())} classes)")
    
    def process_frame(
        self,
        frame: np.ndarray,
        confidence: float = 0.5
    ) -> Tuple[Any, Any]:
        """
        Process frame through both models in parallel.
        
        This is the core method that implements dual-model inference.
        Both models run CONCURRENTLY on separate threads.
        
        Args:
            frame: Input image (H, W, C) as numpy array
            confidence: Detection confidence threshold
            
        Returns:
            Tuple of (guardian_results, learner_results):
                guardian_results: YOLO Results object from Layer 0 (or None)
                learner_results: YOLO Results object from Layer 1 (or None)
        """
        start_time = time.time()
        logger.debug(f"üîÑ [DualYOLO] Processing frame (confidence={confidence})")
        
        # Submit parallel inference tasks
        guardian_future = self.executor.submit(
            self._run_guardian, frame, confidence
        )
        learner_future = self.executor.submit(
            self._run_learner, frame, confidence
        )
        
        # Wait for both models to complete
        guardian_results, guardian_latency = guardian_future.result()
        learner_results, learner_latency = learner_future.result()
        
        logger.debug(f"‚è±Ô∏è [DualYOLO] Guardian: {guardian_latency:.1f}ms, Learner: {learner_latency:.1f}ms")
        
        total_latency = (time.time() - start_time) * 1000  # Convert to ms
        
        # Track performance
        self.guardian_times.append(guardian_latency)
        self.learner_times.append(learner_latency)
        self.total_times.append(total_latency)
        
        # Log performance (every 100 frames)
        if len(self.total_times) % 100 == 0:
            avg_guardian = np.mean(self.guardian_times[-100:])
            avg_learner = np.mean(self.learner_times[-100:])
            avg_total = np.mean(self.total_times[-100:])
            logger.info(f"‚è±Ô∏è Performance (last 100 frames):")
            logger.info(f"   Layer 0: {avg_guardian:.1f}ms (target: <100ms)")
            logger.info(f"   Layer 1: {avg_learner:.1f}ms (target: <150ms)")
            logger.info(f"   Total: {avg_total:.1f}ms (parallel execution)")
        
        return guardian_results, learner_results
    
    def _run_guardian(
        self,
        frame: np.ndarray,
        confidence: float
    ) -> Tuple[Any, float]:
        """
        Run Layer 0 Guardian inference (YOLO11x).
        
        This method runs in a separate thread.
        
        Returns:
            Tuple of (results, latency_ms) where results is YOLO Results object
        """
        start = time.time()
        try:
            # Call model directly to get Results object
            results = self.guardian.model(
                frame,
                conf=confidence,
                verbose=False,
                device=self.guardian.device
            )
            latency = (time.time() - start) * 1000
            logger.debug(f"‚úÖ [Guardian] Inference: {latency:.1f}ms")
            
            # Return first result (single frame inference)
            return results[0] if results else None, latency
        except Exception as e:
            logger.error(f"‚ùå [Guardian] Inference failed: {e}")
            return None, (time.time() - start) * 1000
    
    def _run_learner(
        self,
        frame: np.ndarray,
        confidence: float
    ) -> Tuple[Any, float]:
        """
        Run Layer 1 Learner inference (YOLOE-11s).
        
        This method runs in a separate thread.
        
        Returns:
            Tuple of (results, latency_ms) where results is YOLO Results object
        """
        start = time.time()
        try:
            # Call model directly to get Results object
            results = self.learner.model(
                frame,
                conf=confidence,
                verbose=False,
                device=self.learner.device
            )
            latency = (time.time() - start) * 1000
            logger.debug(f"‚úÖ [Learner] Inference: {latency:.1f}ms")
            
            # Return first result (single frame inference)
            return results[0] if results else None, latency
        except Exception as e:
            logger.error(f"‚ùå [Learner] Inference failed: {e}")
            return None, (time.time() - start) * 1000
    
    def _update_learner_prompts(self) -> None:
        """
        Update YOLOE prompts from adaptive list.
        
        This is called:
        1. During initialization
        2. Every 30 seconds via periodic update
        3. On-demand when new objects added from Gemini/Maps/Memory
        """
        current_prompts = self.prompt_manager.get_current_prompts()
        self.learner.set_classes(current_prompts)
        logger.info(f"üîÑ Updated Layer 1 prompts: {len(current_prompts)} classes")
    
    def add_gemini_objects(self, gemini_response: str) -> List[str]:
        """
        Extract objects from Gemini scene description and add to adaptive list.
        
        Example:
            User: "Describe the scene"
            Gemini: "I see a red fire extinguisher, water fountain, exit signs..."
            ‚Üí Extracts: ["fire extinguisher", "water fountain", "exit sign"]
            ‚Üí Adds to YOLOE prompt list
            ‚Üí Updates model with set_classes()
        
        Args:
            gemini_response: Text response from Gemini API
            
        Returns:
            List of newly added objects
        """
        added_objects = self.prompt_manager.add_from_gemini(gemini_response)
        
        if added_objects:
            logger.info(f"üìù Learned from Gemini: {added_objects}")
            self._update_learner_prompts()  # Immediate update
        
        return added_objects
    
    def add_maps_objects(self, poi_list: List[str]) -> List[str]:
        """
        Convert Google Maps POI to detection objects and add to adaptive list.
        
        Example:
            User location: "Near Starbucks, 123 Main St"
            POI: ["Starbucks", "Bank of America", "CVS Pharmacy"]
            ‚Üí Converts: ["coffee shop sign", "ATM sign", "pharmacy sign"]
            ‚Üí Adds to YOLOE prompt list
            ‚Üí Updates model with set_classes()
        
        Args:
            poi_list: List of nearby POI from Google Maps API
            
        Returns:
            List of newly added objects
        """
        added_objects = self.prompt_manager.add_from_maps(poi_list)
        
        if added_objects:
            logger.info(f"üó∫Ô∏è Learned from Maps: {added_objects}")
            self._update_learner_prompts()  # Immediate update
        
        return added_objects
    
    def add_memory_object(self, object_name: str, metadata: Optional[Dict] = None) -> bool:
        """
        Add user-stored object to adaptive list (Layer 4 Memory integration).
        
        Example:
            User: "Remember my brown leather wallet"
            ‚Üí Adds: "brown leather wallet"
            ‚Üí Adds to YOLOE prompt list
            ‚Üí Updates model with set_classes()
        
        Args:
            object_name: Name of object to remember
            metadata: Optional metadata (color, size, description)
            
        Returns:
            True if added, False if already exists
        """
        added = self.prompt_manager.add_from_memory(object_name, metadata)
        
        if added:
            logger.info(f"üß† Learned from Memory: {object_name}")
            self._update_learner_prompts()  # Immediate update
        
        return added
    
    def get_performance_stats(self) -> Dict[str, float]:
        """
        Get average performance statistics.
        
        Returns:
            Dictionary with average latencies:
            {
                'guardian_avg_ms': float,
                'learner_avg_ms': float,
                'total_avg_ms': float,
                'frames_processed': int
            }
        """
        if not self.total_times:
            return {
                'guardian_avg_ms': 0.0,
                'learner_avg_ms': 0.0,
                'total_avg_ms': 0.0,
                'frames_processed': 0
            }
        
        return {
            'guardian_avg_ms': np.mean(self.guardian_times),
            'learner_avg_ms': np.mean(self.learner_times),
            'total_avg_ms': np.mean(self.total_times),
            'frames_processed': len(self.total_times)
        }
    
    def cleanup(self) -> None:
        """Release resources and shutdown thread pool."""
        logger.info("üßπ Cleaning up Dual YOLO Handler...")
        self.executor.shutdown(wait=True)
        self.guardian.cleanup()
        self.learner.cleanup()
        logger.info("‚úÖ Dual YOLO Handler cleaned up")


# Example usage (for testing):
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # Initialize dual handler
    handler = DualYOLOHandler(device="cpu")  # Use CPU for laptop testing
    
    # Load test frame (simulate camera input)
    import cv2
    test_frame = cv2.imread("tests/test_frame.jpg")
    
    if test_frame is not None:
        # Process frame
        guardian_results, learner_results = handler.process_frame(test_frame)
        
        print(f"\nüõ°Ô∏è Layer 0 Guardian: {len(guardian_results)} safety detections")
        print(f"üéØ Layer 1 Learner: {len(learner_results)} context detections")
        
        # Test adaptive learning from Gemini
        gemini_response = "I see a red fire extinguisher and a water fountain"
        new_objects = handler.add_gemini_objects(gemini_response)
        print(f"\nüìù Learned from Gemini: {new_objects}")
        
        # Test adaptive learning from Maps
        poi_list = ["Starbucks", "Bank of America", "CVS Pharmacy"]
        new_objects = handler.add_maps_objects(poi_list)
        print(f"üó∫Ô∏è Learned from Maps: {new_objects}")
        
        # Get performance stats
        stats = handler.get_performance_stats()
        print(f"\n‚è±Ô∏è Performance Stats:")
        print(f"   Layer 0: {stats['guardian_avg_ms']:.1f}ms")
        print(f"   Layer 1: {stats['learner_avg_ms']:.1f}ms")
        print(f"   Total: {stats['total_avg_ms']:.1f}ms")
    
    # Cleanup
    handler.cleanup()
