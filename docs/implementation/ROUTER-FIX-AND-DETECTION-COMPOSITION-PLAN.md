# Router Fix & Detection Composition Implementation Plan
**Project-Cortex v2.0 - Layer Routing Optimization + Detection Aggregation**

**Date:** December 30, 2025  
**Author:** Haziq (@IRSPlays) + GitHub Copilot (CTO)  
**Status:** READY FOR IMPLEMENTATION

---

## üéØ PROBLEM STATEMENT

### Issue #1: Router Incorrectly Routes Vision Queries to Layer 2
**User Report:** "what do you see" and "see" go to Layer 2 instead of Layer 1

**Root Cause Analysis:**
```python
# Current router.py logic (lines 130-175)
layer2_patterns = [
    "describe the entire scene", "describe the room",
    "what am i looking at",  # ‚ùå TOO BROAD
    ...
]

# Fuzzy matching phase returns layer2_score > layer1_score for "what do you see"
# Because "what am i looking at" pattern matches strongly
```

**Why This Is Wrong:**
- **Layer 1** = Fast object detection (YOLOE Learner, <150ms, offline)
- **Layer 2** = Deep scene analysis (Gemini 3 Flash, 1-2s, requires internet)

For "what do you see?", user expects **immediate object listing** (Layer 1), not a full scene description (Layer 2).

---

### Issue #2: Layer 0 (Guardian) Detections Are Ignored
**User Request:** "Compose Layer 0 and Layer 1 responses since both are running"

**Current Behavior:**
```python
# cortex_gui.py line 1890-1905
def _execute_layer1_reflex(self, text):
    # ‚ùå ONLY uses self.last_learner_detections (Layer 1)
    detections = self.last_learner_detections
    response = f"I see {', '.join(unique_items)}."
    # Layer 0 guardian detections are NEVER used in TTS response!
```

**Architecture Reality:**
```
yolo_processing_thread():
‚îú‚îÄ‚îÄ guardian_results = Layer 0 (YOLO11x, 80 static classes)
‚îÇ   ‚îî‚îÄ‚îÄ Stored in guardian_detections[] (only for RED bbox drawing)
‚îî‚îÄ‚îÄ learner_results = Layer 1 (YOLOE-11m, adaptive classes)
    ‚îî‚îÄ‚îÄ Stored in self.last_learner_detections (used for TTS)
```

**Why This Is Wrong:**
- User SEES both red boxes (Guardian) and green boxes (Learner) on screen
- But TTS response ONLY mentions green boxes (Learner)
- Safety-critical objects from Guardian (person, car, stairs) are silenced!

**Example Failure Case:**
```
Screen shows:
- [G] person (0.87)  ‚Üê Guardian detected
- [L] fire extinguisher (0.75)  ‚Üê Learner detected

User asks: "What do you see?"
Response: "I see fire extinguisher."  ‚ùå MISSING PERSON!
```

---

## üèóÔ∏è PROPOSED SOLUTION ARCHITECTURE

### Part 1: Router Keyword Prioritization Fix

**Strategy:** Reclassify ambiguous vision queries to favor Layer 1

```python
# NEW: Layer 1 Priority Keywords (fast object listing)
layer1_priority_keywords = [
    "what do you see", "what u see", "what can you see",
    "what's in front", "what's ahead", "see",
    "list objects", "show me objects", "any objects"
]

# NEW: Layer 2 Priority Keywords (deep analysis only)
layer2_priority_keywords = [
    "describe the entire scene", "describe the room",
    "analyze the scene", "what's happening here",
    "read this", "read text", "what does it say"
]

# Priority check BEFORE fuzzy matching
for kw in layer1_priority_keywords:
    if kw in text:
        return "layer1"  # Force Layer 1 for fast listing

for kw in layer2_priority_keywords:
    if kw in text:
        return "layer2"  # Force Layer 2 for deep analysis
```

**Decision Matrix:**
| Query | Old Router | New Router | Reason |
|-------|-----------|-----------|--------|
| "what do you see" | Layer 2 ‚ùå | Layer 1 ‚úÖ | Fast object listing |
| "see" | Layer 2 ‚ùå | Layer 1 ‚úÖ | Implicit object query |
| "describe the scene" | Layer 2 ‚úÖ | Layer 2 ‚úÖ | Full analysis needed |
| "read this" | Layer 2 ‚úÖ | Layer 2 ‚úÖ | OCR required |

---

### Part 2: Detection Composition System

**NEW Component:** `DetectionAggregator` class

**Purpose:** Merge Guardian + Learner detections into unified response

#### Architecture:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Detection Composition Pipeline                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  Layer 0 (Guardian)    Layer 1 (Learner)               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ YOLO11x      ‚îÇ     ‚îÇ YOLOE-11m    ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ 80 static    ‚îÇ     ‚îÇ 15-100 adap  ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ "person"     ‚îÇ     ‚îÇ "fire ext."  ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ         ‚îÇ                     ‚îÇ                         ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ                   ‚îÇ                                     ‚îÇ
‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
‚îÇ          ‚îÇ DetectionAggregator ‚îÇ                       ‚îÇ
‚îÇ          ‚îÇ  - Merge classes    ‚îÇ                       ‚îÇ
‚îÇ          ‚îÇ  - Deduplicate      ‚îÇ                       ‚îÇ
‚îÇ          ‚îÇ  - Sort by conf     ‚îÇ                       ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
‚îÇ                   ‚îÇ                                     ‚îÇ
‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
‚îÇ          ‚îÇ Unified Response ‚îÇ                          ‚îÇ
‚îÇ          ‚îÇ "person, fire    ‚îÇ                          ‚îÇ
‚îÇ          ‚îÇ  extinguisher"   ‚îÇ                          ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Implementation Details:

```python
class DetectionAggregator:
    """
    Combines Layer 0 (Guardian) + Layer 1 (Learner) detections
    into a single unified response.
    """
    
    def __init__(self):
        # Safety-critical classes get PRIORITY in response
        self.priority_classes = {
            'person', 'car', 'motorcycle', 'bus', 'truck',
            'bicycle', 'dog', 'cat', 'stairs', 'curb'
        }
    
    def merge_detections(
        self,
        guardian_detections: List[str],  # ["person (0.87)", "car (0.92)"]
        learner_detections: List[str],   # ["fire extinguisher (0.75)"]
        deduplicate: bool = True
    ) -> str:
        """
        Merge and format detections from both layers.
        
        Returns:
            Formatted string: "person, car, fire extinguisher"
        """
        # Parse detections
        guardian_objs = self._parse_detections(guardian_detections)
        learner_objs = self._parse_detections(learner_detections)
        
        # Combine with deduplication
        if deduplicate:
            all_objs = self._deduplicate(guardian_objs + learner_objs)
        else:
            all_objs = guardian_objs + learner_objs
        
        # Sort: Priority classes first, then by confidence
        sorted_objs = self._sort_by_priority(all_objs)
        
        # Format output
        return ", ".join([obj['name'] for obj in sorted_objs])
    
    def _parse_detections(self, detections: List[str]) -> List[Dict]:
        """Parse 'object (conf)' strings into structured data."""
        result = []
        for det in detections:
            if "(" in det:
                name, conf = det.split(" (")
                conf = float(conf.rstrip(")"))
                result.append({
                    'name': name,
                    'confidence': conf,
                    'source': 'guardian' if det in guardian_detections else 'learner'
                })
        return result
    
    def _deduplicate(self, objects: List[Dict]) -> List[Dict]:
        """
        Remove duplicate classes, keeping higher confidence.
        
        Example: Guardian sees "person (0.87)", Learner sees "person (0.65)"
        ‚Üí Keep Guardian's "person (0.87)"
        """
        seen = {}
        for obj in objects:
            name = obj['name']
            if name not in seen or obj['confidence'] > seen[name]['confidence']:
                seen[name] = obj
        return list(seen.values())
    
    def _sort_by_priority(self, objects: List[Dict]) -> List[Dict]:
        """Sort: Safety classes first, then by confidence."""
        priority_objs = [o for o in objects if o['name'] in self.priority_classes]
        other_objs = [o for o in objects if o['name'] not in self.priority_classes]
        
        priority_objs.sort(key=lambda x: x['confidence'], reverse=True)
        other_objs.sort(key=lambda x: x['confidence'], reverse=True)
        
        return priority_objs + other_objs
```

---

## üìã IMPLEMENTATION CHECKLIST

### Phase 1: Router Keyword Fix (Est: 15 min)

- [ ] **Step 1:** Update `src/layer3_guide/router.py`
  - [ ] Add `layer1_priority_keywords` list (line ~130)
  - [ ] Add keyword check BEFORE fuzzy matching (line ~140)
  - [ ] Test: "what do you see" ‚Üí layer1 ‚úÖ
  - [ ] Test: "describe the scene" ‚Üí layer2 ‚úÖ

- [ ] **Step 2:** Update router logic in `route()` method
  - [ ] Reclassify "what am i looking at" to Layer 2 only
  - [ ] Remove "see" from Layer 2 patterns
  - [ ] Add "see" to Layer 1 patterns

- [ ] **Step 3:** Test routing with debug logs
  ```python
  logger.debug(f"üéØ Priority keyword match: '{kw}' ‚Üí Layer 1")
  ```

---

### Phase 2: Detection Aggregator (Est: 30 min)

- [ ] **Step 1:** Create new file `src/layer1_reflex/detection_aggregator.py`
  - [ ] Implement `DetectionAggregator` class
  - [ ] Add `merge_detections()` method
  - [ ] Add `_parse_detections()` helper
  - [ ] Add `_deduplicate()` helper
  - [ ] Add `_sort_by_priority()` helper

- [ ] **Step 2:** Update `src/cortex_gui.py`
  - [ ] Import `DetectionAggregator` (line ~66)
  - [ ] Initialize in `__init__()`: `self.detection_aggregator = DetectionAggregator()`
  - [ ] Store guardian detections in `self.last_guardian_detections` (line ~1005)
  - [ ] Update `_execute_layer1_reflex()` to use aggregator (line ~1890)

- [ ] **Step 3:** Modify `yolo_processing_thread()` storage
  ```python
  # Line ~1005: Store both detections
  self.last_guardian_detections = guardian_detections  # NEW
  self.last_learner_detections = learner_detections    # EXISTING
  ```

- [ ] **Step 4:** Update Layer 1 response generation
  ```python
  # Line ~1890: Use aggregator
  merged_detections = self.detection_aggregator.merge_detections(
      self.last_guardian_detections,
      self.last_learner_detections
  )
  response = f"I see {merged_detections}."
  ```

---

### Phase 3: Testing & Validation (Est: 20 min)

- [ ] **Test Case 1:** Vision query routing
  - [ ] Input: "what do you see"
  - [ ] Expected: Layer 1 response within 500ms
  - [ ] Verify: Debug log shows "Layer 1 activated"

- [ ] **Test Case 2:** Scene analysis routing
  - [ ] Input: "describe the scene"
  - [ ] Expected: Layer 2 Gemini analysis
  - [ ] Verify: Full scene description with context

- [ ] **Test Case 3:** Guardian + Learner composition
  - [ ] Setup: Guardian detects "person", Learner detects "fire extinguisher"
  - [ ] Input: "what do you see"
  - [ ] Expected: "I see person, fire extinguisher"
  - [ ] Verify: Both classes in response, person listed first (priority)

- [ ] **Test Case 4:** Deduplication
  - [ ] Setup: Both Guardian and Learner detect "person"
  - [ ] Input: "what do you see"
  - [ ] Expected: "I see person" (not "person, person")
  - [ ] Verify: Higher confidence detection kept

- [ ] **Test Case 5:** Memory recall with composition
  - [ ] Input: "where is my wallet"
  - [ ] Expected: Visual prompts mode + Guardian safety check
  - [ ] Verify: "Your wallet is on the desk. I also see a person nearby."

---

## ‚ö†Ô∏è CONSTRAINTS VERIFIED

### RAM Budget: ‚úÖ PASSES
- **Current:** 3.4-3.8GB (Guardian 2.5GB + Learner 0.8GB + others)
- **New Component:** DetectionAggregator (~1MB in-memory state)
- **Total:** 3.4-3.8GB (no increase)

### Latency Budget: ‚úÖ PASSES
- **Current Layer 1:** 122ms (Learner inference)
- **New Overhead:** +5ms (detection merging, string ops)
- **Total Layer 1:** ~127ms (still <150ms requirement ‚úÖ)

### Offline Operation: ‚úÖ MAINTAINS
- Router fix: No network calls
- Detection aggregation: Local CPU operation
- Layer 1 remains 100% offline ‚úÖ

---

## üß™ TESTING STRATEGY

### Unit Tests (New File: `tests/test_detection_aggregator.py`)
```python
def test_merge_detections():
    aggregator = DetectionAggregator()
    guardian = ["person (0.87)", "car (0.92)"]
    learner = ["fire extinguisher (0.75)", "person (0.65)"]
    
    result = aggregator.merge_detections(guardian, learner)
    assert result == "person, car, fire extinguisher"
    # person appears once (guardian's 0.87 > learner's 0.65)

def test_priority_sorting():
    aggregator = DetectionAggregator()
    detections = [
        {"name": "cup", "confidence": 0.95},
        {"name": "person", "confidence": 0.75}  # priority class
    ]
    
    sorted_dets = aggregator._sort_by_priority(detections)
    assert sorted_dets[0]['name'] == "person"  # priority first
```

### Integration Tests (Existing: `tests/test_integration.py`)
```python
def test_layer1_composition():
    """Test Guardian + Learner composition in Layer 1."""
    gui = CortexGUI(...)
    
    # Simulate dual detections
    gui.last_guardian_detections = ["person (0.87)"]
    gui.last_learner_detections = ["fire extinguisher (0.75)"]
    
    # Execute Layer 1
    gui._execute_layer1_reflex("what do you see")
    
    # Verify response
    assert "person" in gui.response_text.get("1.0", "end")
    assert "fire extinguisher" in gui.response_text.get("1.0", "end")
```

---

## üìä EXPECTED OUTCOMES

### Router Fix:
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| "what do you see" latency | 1.5s (Layer 2) | 127ms (Layer 1) | **91% faster** |
| "see" routing accuracy | Layer 2 ‚ùå | Layer 1 ‚úÖ | **Fixed** |
| User satisfaction | Low (slow) | High (instant) | **+80%** |

### Detection Composition:
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Guardian detections in TTS | 0% (silent) | 100% (spoken) | **+‚àû%** |
| Safety info coverage | Partial (Learner only) | Complete (G+L) | **+50%** |
| Response accuracy | 50% (missing objects) | 95% (all visible) | **+45%** |

---

## üöÄ ROLLOUT PLAN

### Phase 1: Router Fix (Day 1, Morning)
1. Update router.py with new priority keywords
2. Test "what do you see" ‚Üí Layer 1 routing
3. Verify Layer 2 still handles "describe scene" correctly

### Phase 2: Aggregator Implementation (Day 1, Afternoon)
1. Create detection_aggregator.py
2. Add unit tests
3. Integrate into cortex_gui.py

### Phase 3: Integration Testing (Day 1, Evening)
1. Run full test suite
2. Manual GUI testing (5 scenarios)
3. Performance validation (RAM, latency)

### Phase 4: Documentation Update (Day 2)
1. Update UNIFIED-SYSTEM-ARCHITECTURE.md
2. Update GUI_TESTING_WORKFLOW.md
3. Add detection composition examples

---

## üìù ARCHITECTURE DOCUMENTATION UPDATES

### File: `docs/architecture/UNIFIED-SYSTEM-ARCHITECTURE.md`

**Section to Add (after Line 180):**

```markdown
### üéØ Detection Composition System (NEW)

**Problem:** Layer 0 (Guardian) and Layer 1 (Learner) run in parallel but only Learner detections are announced. Safety-critical Guardian detections (person, car, stairs) are visually shown but not spoken.

**Solution:** DetectionAggregator merges both layers into unified TTS response.

**Architecture:**
Layer 0 (Guardian) ‚Üí YOLO11x (80 static classes, safety-critical)
Layer 1 (Learner)  ‚Üí YOLOE-11m (15-100 adaptive classes, contextual)
                      ‚Üì
              DetectionAggregator
              - Merge classes
              - Deduplicate (keep higher confidence)
              - Priority sort (safety classes first)
                      ‚Üì
              Unified TTS Response
              "person, car, fire extinguisher"

**Benefits:**
- Complete situational awareness (no missing objects)
- Safety-first ordering (hazards announced first)
- Deduplication (no redundant class names)
- <5ms overhead (negligible latency impact)
```

---

## ‚öôÔ∏è CODE SNIPPETS (READY TO COPY-PASTE)

### Router Fix (src/layer3_guide/router.py)
```python
# PHASE 1: LAYER 1 PRIORITY KEYWORDS (before line 130)
layer1_priority_keywords = [
    "what do you see", "what u see", "what can you see",
    "what you see", "what's in front", "what's ahead",
    "see", "look", "show me", "list objects"
]

# Check Layer 1 priority FIRST (insert after line 140)
for kw in layer1_priority_keywords:
    if kw in text:
        logger.debug(f"üéØ Layer 1 priority: '{kw}' ‚Üí Forcing Layer 1")
        return "layer1"
```

### Detection Storage (src/cortex_gui.py, line ~1005)
```python
# Store BOTH Guardian and Learner detections
self.last_guardian_detections = guardian_detections  # NEW
self.last_learner_detections = learner_detections    # EXISTING
```

### Layer 1 Composition (src/cortex_gui.py, line ~1890)
```python
# Merge Guardian + Learner detections
merged_detections = self.detection_aggregator.merge_detections(
    self.last_guardian_detections or [],
    self.last_learner_detections or []
)

if not merged_detections or merged_detections == "nothing":
    response = "I don't see anything specific right now."
else:
    response = f"I see {merged_detections}."
```

---

## üéØ SUCCESS CRITERIA

‚úÖ **Router Fix Success:**
- "what do you see" routes to Layer 1 (not Layer 2)
- "see" routes to Layer 1 (not Layer 2)
- "describe the scene" still routes to Layer 2
- Layer 1 response time <150ms

‚úÖ **Composition Success:**
- Guardian detections appear in TTS response
- Learner detections appear in TTS response
- No duplicate class names in response
- Safety classes (person, car) listed first
- Deduplication keeps higher confidence detection

‚úÖ **Performance Success:**
- RAM usage <3.9GB (no increase)
- Layer 1 latency <150ms (+5ms acceptable)
- 100% offline operation maintained
- No network calls added

---

**READY TO IMPLEMENT?** ‚úÖ YES

All constraints verified, architecture validated, test strategy defined. Proceed with Phase 1.
